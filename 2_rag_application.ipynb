{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RAG (Retrieval-Augmented Generation) Application\n",
    "## Build Production-Ready RAG with Cohere Embeddings + AWS Bedrock\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Document ingestion and chunking strategies\n",
    "- Vector store integration with FAISS\n",
    "- Retrieval-augmented generation using Claude on Bedrock\n",
    "- Context-aware question answering\n",
    "- Citation and source tracking\n",
    "- Evaluation metrics for RAG quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Installation complete\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install boto3 faiss-cpu numpy pandas tiktoken langchain-text-splitters -q\n",
    "\n",
    "print(\"âœ… Installation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Bedrock Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AWS Bedrock configured\n",
      "ğŸ“¦ Embedding Model: cohere.embed-v4:0\n",
      "ğŸ¤– LLM Model: anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    }
   ],
   "source": [
    "# AWS Configuration\n",
    "AWS_REGION = 'us-east-1'\n",
    "EMBEDDING_MODEL_ID = 'cohere.embed-v4:0'\n",
    "LLM_MODEL_ID = 'anthropic.claude-3-sonnet-20240229-v1:0'  # or claude-3-5-sonnet\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "\n",
    "print(f\"âœ… AWS Bedrock configured\")\n",
    "print(f\"ğŸ“¦ Embedding Model: {EMBEDDING_MODEL_ID}\")\n",
    "print(f\"ğŸ¤– LLM Model: {LLM_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data classes defined\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"Represents a document chunk\"\"\"\n",
    "    content: str\n",
    "    metadata: Dict\n",
    "    doc_id: str = \"\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.doc_id:\n",
    "            self.doc_id = f\"doc_{hash(self.content) % 10**8}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    \"\"\"Represents a retrieval result\"\"\"\n",
    "    document: Document\n",
    "    score: float\n",
    "    rank: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RAGResponse:\n",
    "    \"\"\"Represents a complete RAG response\"\"\"\n",
    "    query: str\n",
    "    answer: str\n",
    "    sources: List[RetrievalResult]\n",
    "    context_used: str\n",
    "    timestamp: str\n",
    "\n",
    "print(\"âœ… Data classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TextChunker class defined\n"
     ]
    }
   ],
   "source": [
    "class TextChunker:\n",
    "    \"\"\"Advanced text chunking with multiple strategies\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def chunk_by_tokens(\n",
    "        text: str,\n",
    "        chunk_size: int = 512,\n",
    "        overlap: int = 50\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Chunk text by approximate token count\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            chunk_size: Target chunk size in tokens\n",
    "            overlap: Overlap between chunks\n",
    "        \n",
    "        Returns:\n",
    "            List of text chunks\n",
    "        \"\"\"\n",
    "        # Approximate: 1 token â‰ˆ 4 characters\n",
    "        char_chunk_size = chunk_size * 4\n",
    "        char_overlap = overlap * 4\n",
    "        \n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + char_chunk_size\n",
    "            \n",
    "            # Try to break at sentence boundary\n",
    "            if end < len(text):\n",
    "                # Look for sentence ending\n",
    "                for punct in ['. ', '! ', '? ', '\\n\\n']:\n",
    "                    last_punct = text[start:end].rfind(punct)\n",
    "                    if last_punct != -1:\n",
    "                        end = start + last_punct + len(punct)\n",
    "                        break\n",
    "            \n",
    "            chunk = text[start:end].strip()\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "            \n",
    "            start = end - char_overlap\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def chunk_by_paragraphs(\n",
    "        text: str,\n",
    "        min_chunk_size: int = 100,\n",
    "        max_chunk_size: int = 2000\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Chunk text by paragraphs\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            min_chunk_size: Minimum characters per chunk\n",
    "            max_chunk_size: Maximum characters per chunk\n",
    "        \n",
    "        Returns:\n",
    "            List of text chunks\n",
    "        \"\"\"\n",
    "        paragraphs = re.split(r'\\n\\n+', text)\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_size = 0\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if not para:\n",
    "                continue\n",
    "            \n",
    "            para_size = len(para)\n",
    "            \n",
    "            if current_size + para_size > max_chunk_size and current_chunk:\n",
    "                chunks.append('\\n\\n'.join(current_chunk))\n",
    "                current_chunk = [para]\n",
    "                current_size = para_size\n",
    "            else:\n",
    "                current_chunk.append(para)\n",
    "                current_size += para_size\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append('\\n\\n'.join(current_chunk))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def chunk_by_sentences(\n",
    "        text: str,\n",
    "        sentences_per_chunk: int = 5,\n",
    "        overlap_sentences: int = 1\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Chunk text by sentences\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            sentences_per_chunk: Number of sentences per chunk\n",
    "            overlap_sentences: Overlap in sentences\n",
    "        \n",
    "        Returns:\n",
    "            List of text chunks\n",
    "        \"\"\"\n",
    "        # Simple sentence splitting\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(sentences), sentences_per_chunk - overlap_sentences):\n",
    "            chunk_sentences = sentences[i:i + sentences_per_chunk]\n",
    "            if chunk_sentences:\n",
    "                chunks.append(' '.join(chunk_sentences))\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "print(\"âœ… TextChunker class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RAGVectorStore class defined\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(texts: List[str], input_type: str = \"search_document\") -> np.ndarray:\n",
    "    \"\"\"Generate embeddings using Cohere on Bedrock\"\"\"\n",
    "    body = json.dumps({\n",
    "        \"texts\": texts,\n",
    "        \"input_type\": input_type,\n",
    "        \"embedding_types\": [\"float\"],\n",
    "        \"truncate\": \"END\"\n",
    "    })\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=EMBEDDING_MODEL_ID,\n",
    "        body=body\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response['body'].read())\n",
    "    return np.array(result['embeddings']['float'], dtype=np.float32)\n",
    "\n",
    "\n",
    "class RAGVectorStore:\n",
    "    \"\"\"Vector store optimized for RAG applications\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize without dimension - auto-detect from first batch\"\"\"\n",
    "        self.dimension = None\n",
    "        self.index = None\n",
    "        self.documents: List[Document] = []\n",
    "    \n",
    "    def add_documents(self, documents: List[Document], batch_size: int = 50):\n",
    "        \"\"\"Add documents to the vector store\"\"\"\n",
    "        contents = [doc.content for doc in documents]\n",
    "        \n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(contents), batch_size):\n",
    "            batch = contents[i:i + batch_size]\n",
    "            embeddings = get_embeddings(batch, input_type=\"search_document\")\n",
    "            \n",
    "            # Auto-detect dimension from first batch\n",
    "            if self.index is None:\n",
    "                self.dimension = embeddings.shape[1]\n",
    "                self.index = faiss.IndexFlatIP(self.dimension)\n",
    "                print(f\"âœ… Auto-detected dimension: {self.dimension}\")\n",
    "            \n",
    "            faiss.normalize_L2(embeddings)\n",
    "            all_embeddings.append(embeddings)\n",
    "        \n",
    "        if all_embeddings:\n",
    "            all_embeddings = np.vstack(all_embeddings)\n",
    "            self.index.add(all_embeddings)\n",
    "            self.documents.extend(documents)\n",
    "        \n",
    "        print(f\"âœ… Added {len(documents)} documents\")\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 5,\n",
    "        min_score: float = 0.0\n",
    "    ) -> List[RetrievalResult]:\n",
    "        \"\"\"Search for relevant documents\"\"\"\n",
    "        # Generate query embedding\n",
    "        query_embedding = get_embeddings([query], input_type=\"search_query\")\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # Search\n",
    "        scores, indices = self.index.search(query_embedding, top_k)\n",
    "        \n",
    "        # Prepare results\n",
    "        results = []\n",
    "        for rank, (score, idx) in enumerate(zip(scores[0], indices[0]), 1):\n",
    "            if score >= min_score and idx < len(self.documents):\n",
    "                results.append(RetrievalResult(\n",
    "                    document=self.documents[idx],\n",
    "                    score=float(score),\n",
    "                    rank=rank\n",
    "                ))\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"âœ… RAGVectorStore class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Generation with Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Claude generation function defined\n"
     ]
    }
   ],
   "source": [
    "def generate_with_claude(\n",
    "    prompt: str,\n",
    "    max_tokens: int = 2000,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate response using Claude on Bedrock\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        Generated text\n",
    "    \"\"\"\n",
    "    body = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=LLM_MODEL_ID,\n",
    "        body=body\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response['body'].read())\n",
    "    return result['content'][0]['text']\n",
    "\n",
    "print(\"âœ… Claude generation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG System Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RAGSystem class defined\n"
     ]
    }
   ],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"Complete RAG system with retrieval and generation\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: RAGVectorStore):\n",
    "        self.vector_store = vector_store\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def build_rag_prompt(\n",
    "        self,\n",
    "        query: str,\n",
    "        context_documents: List[RetrievalResult],\n",
    "        include_sources: bool = True\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Build RAG prompt with retrieved context\n",
    "        \n",
    "        Args:\n",
    "            query: User query\n",
    "            context_documents: Retrieved documents\n",
    "            include_sources: Whether to include source references\n",
    "        \n",
    "        Returns:\n",
    "            Formatted prompt\n",
    "        \"\"\"\n",
    "        # Build context section\n",
    "        context_parts = []\n",
    "        for i, result in enumerate(context_documents, 1):\n",
    "            source_info = \"\"\n",
    "            if include_sources and result.document.metadata:\n",
    "                metadata_str = \", \".join(\n",
    "                    f\"{k}: {v}\" for k, v in result.document.metadata.items()\n",
    "                )\n",
    "                source_info = f\" [{metadata_str}]\"\n",
    "            \n",
    "            context_parts.append(\n",
    "                f\"[Document {i}]{source_info}\\n{result.document.content}\"\n",
    "            )\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Build complete prompt\n",
    "        prompt = f\"\"\"You are a helpful AI assistant. Answer the user's question based on the provided context documents.\n",
    "\n",
    "Context Documents:\n",
    "{context}\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Instructions:\n",
    "1. Answer the question using ONLY information from the provided context\n",
    "2. If the context doesn't contain enough information, say so clearly\n",
    "3. Cite which document(s) you used by referencing [Document X]\n",
    "4. Be concise but comprehensive\n",
    "5. If multiple documents contain relevant information, synthesize them\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def query(\n",
    "        self,\n",
    "        question: str,\n",
    "        top_k: int = 3,\n",
    "        min_score: float = 0.3,\n",
    "        temperature: float = 0.7\n",
    "    ) -> RAGResponse:\n",
    "        \"\"\"\n",
    "        Process a query through the RAG pipeline\n",
    "        \n",
    "        Args:\n",
    "            question: User question\n",
    "            top_k: Number of documents to retrieve\n",
    "            min_score: Minimum relevance score\n",
    "            temperature: LLM temperature\n",
    "        \n",
    "        Returns:\n",
    "            RAG response with answer and sources\n",
    "        \"\"\"\n",
    "        # Step 1: Retrieve relevant documents\n",
    "        print(f\"ğŸ” Retrieving top {top_k} relevant documents...\")\n",
    "        retrieved_docs = self.vector_store.search(\n",
    "            question,\n",
    "            top_k=top_k,\n",
    "            min_score=min_score\n",
    "        )\n",
    "        \n",
    "        if not retrieved_docs:\n",
    "            return RAGResponse(\n",
    "                query=question,\n",
    "                answer=\"I couldn't find relevant information to answer your question.\",\n",
    "                sources=[],\n",
    "                context_used=\"\",\n",
    "                timestamp=datetime.now().isoformat()\n",
    "            )\n",
    "        \n",
    "        print(f\"âœ… Retrieved {len(retrieved_docs)} documents\")\n",
    "        \n",
    "        # Step 2: Build prompt with context\n",
    "        prompt = self.build_rag_prompt(question, retrieved_docs)\n",
    "        \n",
    "        # Step 3: Generate answer\n",
    "        print(\"ğŸ¤– Generating answer...\")\n",
    "        answer = generate_with_claude(prompt, temperature=temperature)\n",
    "        \n",
    "        # Step 4: Create response\n",
    "        response = RAGResponse(\n",
    "            query=question,\n",
    "            answer=answer,\n",
    "            sources=retrieved_docs,\n",
    "            context_used=prompt,\n",
    "            timestamp=datetime.now().isoformat()\n",
    "        )\n",
    "        \n",
    "        # Store in history\n",
    "        self.conversation_history.append(response)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def display_response(self, response: RAGResponse):\n",
    "        \"\"\"Display formatted RAG response\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"QUESTION: {response.query}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nANSWER:\\n{response.answer}\")\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(f\"SOURCES ({len(response.sources)} documents):\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for source in response.sources:\n",
    "            print(f\"\\n[{source.rank}] Score: {source.score:.4f}\")\n",
    "            if source.document.metadata:\n",
    "                print(f\"    Metadata: {source.document.metadata}\")\n",
    "            print(f\"    Content: {source.document.content[:150]}...\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"âœ… RAGSystem class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Knowledge Base: AWS Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded AWS documentation (3405 characters)\n"
     ]
    }
   ],
   "source": [
    "# Sample AWS documentation content\n",
    "aws_documentation = \"\"\"\n",
    "Amazon Bedrock Overview\n",
    "\n",
    "Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.\n",
    "\n",
    "With Amazon Bedrock, you can easily experiment with and evaluate top foundation models for your use case, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources.\n",
    "\n",
    "Amazon SageMaker Features\n",
    "\n",
    "Amazon SageMaker is a fully managed machine learning service. With SageMaker, data scientists and developers can quickly and easily build and train machine learning models, and then directly deploy them into a production-ready hosted environment.\n",
    "\n",
    "SageMaker provides an integrated Jupyter authoring notebook instance for easy access to your data sources for exploration and analysis, so you don't have to manage servers. It also provides common machine learning algorithms that are optimized to run efficiently against extremely large data in a distributed environment.\n",
    "\n",
    "AWS Lambda Functions\n",
    "\n",
    "AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers, creating workload-aware cluster scaling logic, maintaining event integrations, or managing runtimes.\n",
    "\n",
    "With Lambda, you can run code for virtually any type of application or backend service - all with zero administration. Just upload your code as a ZIP file or container image, and Lambda automatically and precisely allocates compute execution power and runs your code based on the incoming request or event, for any scale of traffic.\n",
    "\n",
    "Amazon S3 Storage\n",
    "\n",
    "Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can use Amazon S3 to store and protect any amount of data for a range of use cases.\n",
    "\n",
    "Amazon S3 provides management features so that you can optimize, organize, and configure access to your data to meet your specific business, organizational, and compliance requirements.\n",
    "\n",
    "Amazon EC2 Instances\n",
    "\n",
    "Amazon Elastic Compute Cloud (Amazon EC2) provides scalable computing capacity in the Amazon Web Services (AWS) Cloud. Using Amazon EC2 eliminates your need to invest in hardware up front, so you can develop and deploy applications faster.\n",
    "\n",
    "You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage. Amazon EC2 enables you to scale up or down to handle changes in requirements or spikes in popularity, reducing your need to forecast traffic.\n",
    "\n",
    "Amazon VPC Networking\n",
    "\n",
    "Amazon Virtual Private Cloud (Amazon VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment.\n",
    "\n",
    "With Amazon VPC, you can select your own IP address range, create subnets, and configure route tables and network gateways. You can use both IPv4 and IPv6 for most resources in your VPC, helping to ensure secure and easy access to resources and applications.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"âœ… Loaded AWS documentation ({len(aws_documentation)} characters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RAG Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Building RAG Knowledge Base\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ Chunking documentation...\n",
      "âœ… Created 4 chunks\n",
      "\n",
      "ğŸ—ï¸ Initializing vector store...\n",
      "\n",
      "ğŸ“Š Generating embeddings and building index...\n",
      "âœ… Auto-detected dimension: 1536\n",
      "âœ… Added 4 documents\n",
      "\n",
      "âœ… Initializing RAG system...\n",
      "\n",
      "================================================================================\n",
      "âœ… RAG Knowledge Base Ready!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Building RAG Knowledge Base\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 1: Chunk the documentation\n",
    "print(\"\\nğŸ“„ Chunking documentation...\")\n",
    "chunker = TextChunker()\n",
    "chunks = chunker.chunk_by_paragraphs(\n",
    "    aws_documentation,\n",
    "    min_chunk_size=200,\n",
    "    max_chunk_size=1000\n",
    ")\n",
    "\n",
    "print(f\"âœ… Created {len(chunks)} chunks\")\n",
    "\n",
    "# Step 2: Create Document objects\n",
    "documents = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # Extract service name from chunk\n",
    "    first_line = chunk.split('\\n')[0]\n",
    "    \n",
    "    doc = Document(\n",
    "        content=chunk,\n",
    "        metadata={\n",
    "            'source': 'AWS Documentation',\n",
    "            'chunk_id': i,\n",
    "            'title': first_line[:50] if first_line else f'Chunk {i}'\n",
    "        }\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "# Step 3: Initialize vector store\n",
    "print(\"\\nğŸ—ï¸ Initializing vector store...\")\n",
    "vector_store = RAGVectorStore()\n",
    "\n",
    "# Step 4: Add documents\n",
    "print(\"\\nğŸ“Š Generating embeddings and building index...\")\n",
    "vector_store.add_documents(documents, batch_size=10)\n",
    "\n",
    "# Step 5: Initialize RAG system\n",
    "print(\"\\nâœ… Initializing RAG system...\")\n",
    "rag_system = RAGSystem(vector_store)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… RAG Knowledge Base Ready!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "TEST QUERY 1\n",
      "################################################################################\n",
      "ğŸ” Retrieving top 2 relevant documents...\n",
      "âœ… Retrieved 2 documents\n",
      "ğŸ¤– Generating answer...\n",
      "\n",
      "================================================================================\n",
      "QUESTION: What is Amazon Bedrock and what capabilities does it provide?\n",
      "================================================================================\n",
      "\n",
      "ANSWER:\n",
      "According to [Document 1], Amazon Bedrock is a fully managed service that provides access to high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API. It offers capabilities to:\n",
      "\n",
      "1. Easily experiment with and evaluate top foundation models for different use cases.\n",
      "2. Privately customize the foundation models with your own data using techniques like fine-tuning and Retrieval Augmented Generation (RAG).\n",
      "3. Build agents that can execute tasks using your enterprise systems and data sources.\n",
      "4. Leverage security, privacy, and responsible AI features provided by Amazon Bedrock.\n",
      "\n",
      "The context documents do not provide further details on the specific capabilities of Amazon Bedrock beyond what is mentioned in [Document 1]. The other documents are about Amazon S3 storage service and Amazon EC2 compute instances, which are not directly relevant to answering this question.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCES (2 documents):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[1] Score: 0.5941\n",
      "    Metadata: {'source': 'AWS Documentation', 'chunk_id': 0, 'title': 'Amazon Bedrock Overview'}\n",
      "    Content: Amazon Bedrock Overview\n",
      "\n",
      "Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI com...\n",
      "\n",
      "[2] Score: 0.4348\n",
      "    Metadata: {'source': 'AWS Documentation', 'chunk_id': 2, 'title': 'Amazon Simple Storage Service (Amazon S3) is an ob'}\n",
      "    Content: Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and perf...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Query 1: Specific service question\n",
    "query1 = \"What is Amazon Bedrock and what capabilities does it provide?\"\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"TEST QUERY 1\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "response1 = rag_system.query(query1, top_k=2)\n",
    "rag_system.display_response(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "TEST QUERY 2\n",
      "################################################################################\n",
      "ğŸ” Retrieving top 3 relevant documents...\n",
      "âœ… Retrieved 3 documents\n",
      "ğŸ¤– Generating answer...\n",
      "\n",
      "================================================================================\n",
      "QUESTION: What are the differences between Lambda and EC2?\n",
      "================================================================================\n",
      "\n",
      "ANSWER:\n",
      "The key differences between AWS Lambda and Amazon EC2 based on the provided context are:\n",
      "\n",
      "1. Compute model: Lambda is a serverless compute service where you run code without provisioning servers, while EC2 provides virtual servers that you manage and scale yourself [Document 1, Document 2].\n",
      "\n",
      "2. Pricing: With Lambda, you pay only for the compute time used when your code runs. With EC2, you pay for the instances whether they are being utilized or not [Document 2].\n",
      "\n",
      "3. Scalability: Lambda automatically scales the compute capacity based on incoming requests or events. With EC2, you need to manually scale instances up or down based on demand [Document 2].\n",
      "\n",
      "4. Management: Lambda requires zero administration as AWS manages all operational responsibilities. With EC2, you are responsible for managing the virtual servers, including security patching, monitoring, etc. [Document 1, Document 2].\n",
      "\n",
      "The context documents provide a good overview of the key differences between these two AWS compute services in terms of their compute model, pricing, scalability, and management responsibilities. However, they do not go into more detailed technical differences or use cases.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCES (3 documents):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[1] Score: 0.5124\n",
      "    Metadata: {'source': 'AWS Documentation', 'chunk_id': 1, 'title': 'SageMaker provides an integrated Jupyter authoring'}\n",
      "    Content: SageMaker provides an integrated Jupyter authoring notebook instance for easy access to your data sources for exploration and analysis, so you don't h...\n",
      "\n",
      "[2] Score: 0.5053\n",
      "    Metadata: {'source': 'AWS Documentation', 'chunk_id': 2, 'title': 'Amazon Simple Storage Service (Amazon S3) is an ob'}\n",
      "    Content: Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and perf...\n",
      "\n",
      "[3] Score: 0.3689\n",
      "    Metadata: {'source': 'AWS Documentation', 'chunk_id': 3, 'title': 'Amazon VPC Networking'}\n",
      "    Content: Amazon VPC Networking\n",
      "\n",
      "Amazon Virtual Private Cloud (Amazon VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Query 2: Comparison question\n",
    "query2 = \"What are the differences between Lambda and EC2?\"\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"TEST QUERY 2\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "response2 = rag_system.query(query2, top_k=3)\n",
    "rag_system.display_response(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "TEST QUERY 3\n",
      "################################################################################\n",
      "ğŸ” Retrieving top 3 relevant documents...\n",
      "âœ… Retrieved 3 documents\n",
      "ğŸ¤– Generating answer...\n",
      "\n",
      "================================================================================\n",
      "QUESTION: How can I use SageMaker with S3 for machine learning?\n",
      "================================================================================\n",
      "\n",
      "ANSWER:\n",
      "According to the provided context documents, you can use Amazon SageMaker with Amazon S3 for machine learning in the following ways:\n",
      "\n",
      "1. SageMaker provides an integrated Jupyter notebook instance that allows easy access to your data sources, which can include data stored in Amazon S3, for exploration and analysis [Document 1].\n",
      "\n",
      "2. SageMaker offers optimized machine learning algorithms that can efficiently run against extremely large datasets stored in Amazon S3 in a distributed environment [Document 1].\n",
      "\n",
      "3. You can use SageMaker to quickly build, train, and deploy machine learning models, likely leveraging data stored in Amazon S3 [Document 2].\n",
      "\n",
      "However, the context does not provide specific details on how to integrate SageMaker with S3 for machine learning tasks. It mainly highlights that SageMaker can access and work with data stored in S3 for various machine learning purposes.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCES (3 documents):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[1] Score: 0.5203\n",
      "    Metadata: {'source': 'AWS Documentation', 'chunk_id': 1, 'title': 'SageMaker provides an integrated Jupyter authoring'}\n",
      "    Content: SageMaker provides an integrated Jupyter authoring notebook instance for easy access to your data sources for exploration and analysis, so you don't h...\n",
      "\n",
      "[2] Score: 0.4384\n",
      "    Metadata: {'source': 'AWS Documentation', 'chunk_id': 0, 'title': 'Amazon Bedrock Overview'}\n",
      "    Content: Amazon Bedrock Overview\n",
      "\n",
      "Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI com...\n",
      "\n",
      "[3] Score: 0.3962\n",
      "    Metadata: {'source': 'AWS Documentation', 'chunk_id': 2, 'title': 'Amazon Simple Storage Service (Amazon S3) is an ob'}\n",
      "    Content: Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and perf...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Query 3: Multi-hop reasoning\n",
    "query3 = \"How can I use SageMaker with S3 for machine learning?\"\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"TEST QUERY 3\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "response3 = rag_system.query(query3, top_k=3)\n",
    "rag_system.display_response(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "TEST QUERY 4 (Out-of-Domain)\n",
      "################################################################################\n",
      "ğŸ” Retrieving top 2 relevant documents...\n",
      "\n",
      "================================================================================\n",
      "QUESTION: How do I cook pasta?\n",
      "================================================================================\n",
      "\n",
      "ANSWER:\n",
      "I couldn't find relevant information to answer your question.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCES (0 documents):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Query 4: Out-of-domain question\n",
    "query4 = \"How do I cook pasta?\"\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"TEST QUERY 4 (Out-of-Domain)\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "response4 = rag_system.query(query4, top_k=2, min_score=0.5)\n",
    "rag_system.display_response(response4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RAG EVALUATION METRICS\n",
      "================================================================================\n",
      "\n",
      "Query 1: What is Amazon Bedrock and what capabilities does it provide...\n",
      "--------------------------------------------------------------------------------\n",
      "  has_citations: True\n",
      "  has_uncertainty: False\n",
      "  answer_length_words: 144\n",
      "  num_sources_retrieved: 2\n",
      "  avg_source_score: 0.5144682675600052\n",
      "\n",
      "Query 2: What are the differences between Lambda and EC2?...\n",
      "--------------------------------------------------------------------------------\n",
      "  has_citations: True\n",
      "  has_uncertainty: False\n",
      "  answer_length_words: 180\n",
      "  num_sources_retrieved: 3\n",
      "  avg_source_score: 0.46221161882082623\n",
      "\n",
      "Query 3: How can I use SageMaker with S3 for machine learning?...\n",
      "--------------------------------------------------------------------------------\n",
      "  has_citations: True\n",
      "  has_uncertainty: False\n",
      "  answer_length_words: 137\n",
      "  num_sources_retrieved: 3\n",
      "  avg_source_score: 0.4516541262467702\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "class RAGEvaluator:\n",
    "    \"\"\"Evaluate RAG system performance\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_retrieval_metrics(\n",
    "        retrieved_docs: List[RetrievalResult],\n",
    "        relevant_doc_ids: List[str]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate retrieval metrics\n",
    "        \n",
    "        Args:\n",
    "            retrieved_docs: Retrieved documents\n",
    "            relevant_doc_ids: Ground truth relevant document IDs\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of metrics\n",
    "        \"\"\"\n",
    "        retrieved_ids = {doc.document.doc_id for doc in retrieved_docs}\n",
    "        relevant_set = set(relevant_doc_ids)\n",
    "        \n",
    "        # True positives\n",
    "        tp = len(retrieved_ids & relevant_set)\n",
    "        \n",
    "        # Precision: TP / (TP + FP)\n",
    "        precision = tp / len(retrieved_ids) if retrieved_ids else 0\n",
    "        \n",
    "        # Recall: TP / (TP + FN)\n",
    "        recall = tp / len(relevant_set) if relevant_set else 0\n",
    "        \n",
    "        # F1 Score\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Mean Reciprocal Rank (MRR)\n",
    "        mrr = 0\n",
    "        for rank, doc in enumerate(retrieved_docs, 1):\n",
    "            if doc.document.doc_id in relevant_set:\n",
    "                mrr = 1 / rank\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'mrr': mrr\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_answer_quality(\n",
    "        response: RAGResponse,\n",
    "        check_citation: bool = True\n",
    "    ) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Evaluate answer quality\n",
    "        \n",
    "        Args:\n",
    "            response: RAG response\n",
    "            check_citation: Whether to check for citations\n",
    "        \n",
    "        Returns:\n",
    "            Quality metrics\n",
    "        \"\"\"\n",
    "        answer = response.answer\n",
    "        \n",
    "        # Check for citations\n",
    "        has_citations = bool(re.search(r'\\[Document \\d+\\]', answer))\n",
    "        \n",
    "        # Check for uncertainty markers\n",
    "        uncertainty_phrases = [\n",
    "            \"I don't know\",\n",
    "            \"not enough information\",\n",
    "            \"can't find\",\n",
    "            \"unable to answer\"\n",
    "        ]\n",
    "        has_uncertainty = any(phrase.lower() in answer.lower() for phrase in uncertainty_phrases)\n",
    "        \n",
    "        # Answer length\n",
    "        answer_length = len(answer.split())\n",
    "        \n",
    "        # Number of sources used\n",
    "        num_sources = len(response.sources)\n",
    "        \n",
    "        return {\n",
    "            'has_citations': has_citations,\n",
    "            'has_uncertainty': has_uncertainty,\n",
    "            'answer_length_words': answer_length,\n",
    "            'num_sources_retrieved': num_sources,\n",
    "            'avg_source_score': np.mean([s.score for s in response.sources]) if response.sources else 0\n",
    "        }\n",
    "\n",
    "# Evaluate responses\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RAG EVALUATION METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "evaluator = RAGEvaluator()\n",
    "\n",
    "for i, response in enumerate([response1, response2, response3], 1):\n",
    "    print(f\"\\nQuery {i}: {response.query[:60]}...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    quality_metrics = evaluator.evaluate_answer_quality(response)\n",
    "    \n",
    "    for metric, value in quality_metrics.items():\n",
    "        print(f\"  {metric}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation History Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONVERSATION HISTORY SUMMARY\n",
      "================================================================================\n",
      "                                                Query  Num Sources Avg Score  Answer Length           Timestamp\n",
      "What is Amazon Bedrock and what capabilities does ...            2     0.514            144 2026-01-31T16:39:06\n",
      "  What are the differences between Lambda and EC2?...            3     0.462            180 2026-01-31T16:39:16\n",
      "How can I use SageMaker with S3 for machine learni...            3     0.452            137 2026-01-31T16:39:19\n"
     ]
    }
   ],
   "source": [
    "# Analyze conversation history\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONVERSATION HISTORY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if rag_system.conversation_history:\n",
    "    df_history = pd.DataFrame([\n",
    "        {\n",
    "            'Query': resp.query[:50] + '...',\n",
    "            'Num Sources': len(resp.sources),\n",
    "            'Avg Score': f\"{np.mean([s.score for s in resp.sources]):.3f}\" if resp.sources else 'N/A',\n",
    "            'Answer Length': len(resp.answer.split()),\n",
    "            'Timestamp': resp.timestamp[:19]\n",
    "        }\n",
    "        for resp in rag_system.conversation_history\n",
    "    ])\n",
    "    \n",
    "    print(df_history.to_string(index=False))\n",
    "else:\n",
    "    print(\"No conversation history available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "                    RAG APPLICATION SUMMARY\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "âœ… SUCCESSFULLY IMPLEMENTED:\n",
      "   â€¢ Document chunking with multiple strategies\n",
      "   â€¢ Vector store with FAISS for efficient retrieval\n",
      "   â€¢ Cohere embeddings for semantic search\n",
      "   â€¢ Claude 3 for contextual answer generation\n",
      "   â€¢ Citation and source tracking\n",
      "   â€¢ Conversation history management\n",
      "   â€¢ RAG evaluation metrics\n",
      "\n",
      "ğŸ¯ RAG PIPELINE:\n",
      "   1. Document Ingestion â†’ Chunking\n",
      "   2. Embedding Generation â†’ Vector Store\n",
      "   3. Query â†’ Semantic Retrieval\n",
      "   4. Context Building â†’ LLM Generation\n",
      "   5. Response â†’ Source Attribution\n",
      "\n",
      "ğŸ“Š KEY COMPONENTS:\n",
      "   â€¢ TextChunker: Multiple chunking strategies\n",
      "   â€¢ RAGVectorStore: FAISS-based vector search\n",
      "   â€¢ RAGSystem: Complete pipeline orchestration\n",
      "   â€¢ RAGEvaluator: Quality metrics\n",
      "\n",
      "ğŸ’¡ BEST PRACTICES:\n",
      "   1. Chunk documents at semantic boundaries (paragraphs/sections)\n",
      "   2. Use appropriate chunk size (200-1000 characters)\n",
      "   3. Include chunk overlap for context continuity\n",
      "   4. Set minimum relevance threshold (0.3-0.5)\n",
      "   5. Retrieve 3-5 documents for context\n",
      "   6. Include source citations in prompts\n",
      "   7. Monitor retrieval quality metrics\n",
      "   8. Implement fallback for low-confidence answers\n",
      "\n",
      "ğŸ”§ OPTIMIZATION OPPORTUNITIES:\n",
      "   â€¢ Implement hybrid search (dense + sparse)\n",
      "   â€¢ Add reranking layer\n",
      "   â€¢ Cache frequent queries\n",
      "   â€¢ Implement query expansion\n",
      "   â€¢ Add multi-turn conversation support\n",
      "   â€¢ Implement streaming responses\n",
      "\n",
      "ğŸ“ˆ PRODUCTION CONSIDERATIONS:\n",
      "   â€¢ Scale vector store for large document sets\n",
      "   â€¢ Implement incremental indexing\n",
      "   â€¢ Add monitoring and logging\n",
      "   â€¢ Implement rate limiting\n",
      "   â€¢ Add error handling and retries\n",
      "   â€¢ Consider cost optimization\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                    RAG APPLICATION SUMMARY\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "âœ… SUCCESSFULLY IMPLEMENTED:\n",
    "   â€¢ Document chunking with multiple strategies\n",
    "   â€¢ Vector store with FAISS for efficient retrieval\n",
    "   â€¢ Cohere embeddings for semantic search\n",
    "   â€¢ Claude 3 for contextual answer generation\n",
    "   â€¢ Citation and source tracking\n",
    "   â€¢ Conversation history management\n",
    "   â€¢ RAG evaluation metrics\n",
    "\n",
    "ğŸ¯ RAG PIPELINE:\n",
    "   1. Document Ingestion â†’ Chunking\n",
    "   2. Embedding Generation â†’ Vector Store\n",
    "   3. Query â†’ Semantic Retrieval\n",
    "   4. Context Building â†’ LLM Generation\n",
    "   5. Response â†’ Source Attribution\n",
    "\n",
    "ğŸ“Š KEY COMPONENTS:\n",
    "   â€¢ TextChunker: Multiple chunking strategies\n",
    "   â€¢ RAGVectorStore: FAISS-based vector search\n",
    "   â€¢ RAGSystem: Complete pipeline orchestration\n",
    "   â€¢ RAGEvaluator: Quality metrics\n",
    "\n",
    "ğŸ’¡ BEST PRACTICES:\n",
    "   1. Chunk documents at semantic boundaries (paragraphs/sections)\n",
    "   2. Use appropriate chunk size (200-1000 characters)\n",
    "   3. Include chunk overlap for context continuity\n",
    "   4. Set minimum relevance threshold (0.3-0.5)\n",
    "   5. Retrieve 3-5 documents for context\n",
    "   6. Include source citations in prompts\n",
    "   7. Monitor retrieval quality metrics\n",
    "   8. Implement fallback for low-confidence answers\n",
    "\n",
    "ğŸ”§ OPTIMIZATION OPPORTUNITIES:\n",
    "   â€¢ Implement hybrid search (dense + sparse)\n",
    "   â€¢ Add reranking layer\n",
    "   â€¢ Cache frequent queries\n",
    "   â€¢ Implement query expansion\n",
    "   â€¢ Add multi-turn conversation support\n",
    "   â€¢ Implement streaming responses\n",
    "\n",
    "ğŸ“ˆ PRODUCTION CONSIDERATIONS:\n",
    "   â€¢ Scale vector store for large document sets\n",
    "   â€¢ Implement incremental indexing\n",
    "   â€¢ Add monitoring and logging\n",
    "   â€¢ Implement rate limiting\n",
    "   â€¢ Add error handling and retries\n",
    "   â€¢ Consider cost optimization\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
